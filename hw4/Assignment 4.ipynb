{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS247 Advanced Data Mining - Assignment 4\n",
    "## Deadline: 11:59PM, February 16, 2023\n",
    "\n",
    "## Instructions\n",
    "Each assignment is structured as a Jupyter notebook, offering interactive tutorials that align with our lectures. You will encounter two types of problems: *write-up problems* and *coding problems*.\n",
    "\n",
    "1. **Write-up Problems:** These problems are primarily theoretical, requiring you to demonstrate your understanding of lecture concepts and to provide mathematical proofs for key theorems. Your answers should include sufficient steps for the mathematical derivations.\n",
    "2. **Coding Problems:** Here, you will be engaging with practical coding tasks. These may involve completing code segments provided in the notebooks or developing models from scratch.\n",
    "\n",
    "To ensure clarity and consistency in your submissions, please adhere to the following guidelines:\n",
    "\n",
    "* For write-up problems, use Markdown bullet points to format text answers. Also, express all mathematical equations using $\\LaTeX$ and avoid plain text such as `x0`, `x^1`, or `R x Q` for equations.\n",
    "* For coding problems, comment on your code thoroughly for readability and ensure your code is executable. Non-runnable code may lead to a loss of **all** points. Coding problems have automated grading, and altering the grading code will result in a deduction of **all** points.\n",
    "* Your submission should show the entire process of data loading, preprocessing, model implementation, training, and result analysis. This can be achieved through a mix of explanatory text cells, inline comments, intermediate result displays, and experimental visualizations.\n",
    "\n",
    "### Submission Requirements\n",
    "\n",
    "* Submit your solutions through GradeScope in BruinLearn.\n",
    "* Late submissions are allowed up to 24 hours post-deadline with a penalty factor of $\\mathbf{1}(t\\leq24)e^{-(\\ln(2)/12)t}$.\n",
    "\n",
    "### Collaboration and Integrity\n",
    "\n",
    "* Collaboration is encouraged, but all final submissions must be your own work. Please acknowledge any collaboration or external sources used, including websites, papers, and GitHub repositories.\n",
    "* Any suspicious cases of academic misconduct will be reported to The Office of the Dean of Students.\n",
    "\n",
    "## Outline\n",
    "* Part 1: The Transformer Model (80 points)\n",
    "* Part 2: Spectral Clustering (30 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: The Transformer Model (80 points)\n",
    "\n",
    "In this assignment, you will build a Transformer model from scratch using PyTorch. This exercise aims to deepen your understanding of the Transformer architecture, as introduced by Vaswani et al. in the landmark paper [*Attention is All You Need*](https://arxiv.org/abs/1706.03762). By implementing the various components of the Transformer, you will gain hands-on experience with key concepts such as self-attention mechanisms, positional encoding, and the overall architecture of the Transformer model.\n",
    "\n",
    "- This Jupyter Notebook contains template code and instructions for implementing various parts of the Transformer model.\n",
    "- Follow the instructions and complete the code in the cells marked `# TODO`.\n",
    "- Make sure to read the comments carefully to understand what each part of the code should do.\n",
    "- You can refer to the original \"Attention is All You Need\" paper and the PyTorch documentation for guidance.\n",
    "- After implementing the components, you will train your Transformer model on a sample dataset to see it in action.\n",
    "\n",
    "### Key Components of the Transformer\n",
    "\n",
    "<img src=\"transformer.svg\" width=\"30%\" />\n",
    "\n",
    "1. **Encoder and Decoder**: The Transformer model consists of an encoder to process the input text and a decoder to generate the output text. Both the encoder and decoder are composed of multiple layers that contain self-attention and feed-forward neural network components.\n",
    "2. **Multi-Head Attention**: This component allows the model to jointly attend to information from different representation subspaces at different positions. Implementing multi-head attention is a critical part of this assignment.\n",
    "3. **Positional Encoding**: Since the model contains no recurrence or convolution, positional encodings are added to give the model some information about the relative or absolute position of the tokens in the sequence.\n",
    "4. **Feed-Forward Networks**: Each layer of the encoder and decoder contains a feed-forward neural network which applies two linear transformations and a ReLU activation in between.\n",
    "\n",
    "In the following sections, you will implement these components step by step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GPU Support\n",
    "\n",
    "Considering the size of the training data, it is strongly suggested to use [Google Colab](https://colab.research.google.com/) or a GPU server for this exercise. If you are using Colab, you can manually switch to a CPU device on Colab by clicking `Runtime -> Change runtime type` and selecting `GPU` under `Hardware Accelerator`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "USE_GPU = True\n",
    "\n",
    "if USE_GPU and torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "elif USE_GPU and torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "else:\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Multi-Head Attention (20 points)\n",
    "\n",
    "The attention mechanism computes the dot product between the query and key vectors, scaled by the square root of the dimension of the key vectors. The attention weights are then used to compute a weighted sum of the value vectors.\n",
    "Please implement the `scaled_dot_product_attention` function at first, which will used as a building block for the multi-head attention mechanism. This function computes the attention weights and the weighted sum of the value vectors, given the projected query, key, and value vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(query, key, value, mask=None):\n",
    "    \"\"\"\n",
    "    Compute the scaled dot product attention.\n",
    "\n",
    "    Parameters:\n",
    "    - query (torch.Tensor): Queries tensor with shape (batch_size, num_heads, seq_len_q, depth).\n",
    "    - key (torch.Tensor): Keys tensor with shape (batch_size, num_heads, seq_len_k, depth).\n",
    "    - value (torch.Tensor): Values tensor with shape (batch_size, num_heads, seq_len_v, depth).\n",
    "    - mask (torch.Tensor, optional): Mask tensor to filter out certain positions before\n",
    "      applying softmax. The mask's shape is broadcastable to (batch_size, num_heads, seq_len_q, seq_len_k).\n",
    "      The mask will contain either 0 values to indicate that the corresponding token in the input sequence\n",
    "      should be considered in the computations or a 1 to indicate otherwise.\n",
    "\n",
    "    Returns:\n",
    "    - torch.Tensor: The output after applying attention to the value vector. Shape is (batch_size, num_heads, seq_len_q, depth).\n",
    "    - torch.Tensor: The attention weights. Shape is (batch_size, num_heads, seq_len_q, seq_len_k).\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO: Compute the matrix multiplication between the query and key tensors\n",
    "    # Hint: The resulting tensor has shape (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "    matmul_qk = None\n",
    "    assert matmul_qk.size() == (query.size(0), query.size(1), query.size(2), key.size(2))\n",
    "    \n",
    "    d_k = query.size(-1)\n",
    "    # TODO: Scale the attention weights by the dimension of the key\n",
    "    scaled_attention_logits = None\n",
    "\n",
    "    # TODO: Apply the mask to the scaled tensor\n",
    "    # Hint: You can use the `masked_fill` method of the tensor and mask out\n",
    "    #   certain positions to a large negative number (e.g., -1e9)\n",
    "    #   such that the attention weights will be zero after applying softmax\n",
    "    if mask is not None:\n",
    "        pass\n",
    "\n",
    "    # TODO: Apply the softmax function to obtain the attention weights\n",
    "    attention_weights = None\n",
    "\n",
    "    # TODO: Apply the attention weights to the value tensor\n",
    "    output = None\n",
    "    return output, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can verify your implementation by running the test cases provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "num_heads = 8\n",
    "seq_len_q = 10\n",
    "seq_len_k = 10\n",
    "seq_len_v = 10\n",
    "depth = 128\n",
    "\n",
    "query = torch.rand(batch_size, num_heads, seq_len_q, depth)\n",
    "key = torch.rand(batch_size, num_heads, seq_len_k, depth)\n",
    "value = torch.rand(batch_size, num_heads, seq_len_v, depth)\n",
    "\n",
    "output, attention_weights = scaled_dot_product_attention(query, key, value)\n",
    "\n",
    "assert output.shape == (batch_size, num_heads, seq_len_q, depth)\n",
    "assert attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "\n",
    "from torch.nn.functional import scaled_dot_product_attention as torch_scaled_dot_product_attention\n",
    "from torch.testing import assert_close\n",
    "\n",
    "torch_output = torch_scaled_dot_product_attention(query, key, value)\n",
    "assert_close(output, torch_output, rtol=1e-6, atol=1e-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Transformers, there are multiple \"attention heads\", each of which captures a different aspect of the input.\n",
    "At first, the query and key vectors are passed through a linear layer to project them to a higher-dimensional space. Then, the scaled dot-product attention is applied to each of these projected versions of the query and key vectors. The output of the linear layer is then reshaped to split the attention heads. The attention weights are computed for each head, and the weighted sum is then concatenated and passed through another linear layer to produce the final output.\n",
    "Please implement the `MultiHeadAttention` class, which contains the logic for the multi-head attention mechanism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-Head Attention module takes in queries, keys, and values, and\n",
    "    performs scaled dot-product attention on them. This implementation also\n",
    "    supports masking irrelevant positions.\n",
    "    \n",
    "    Parameters:\n",
    "    - d_model (int): The dimension of the input embedding vectors.\n",
    "    - num_heads (int): The number of attention heads. d_model must be divisible by num_heads.\n",
    "\n",
    "    The architecture splits the input embedding vector into multiple heads because\n",
    "    it allows the model to jointly attend to information from different representation\n",
    "    subspaces at different positions. After the attention is applied, the heads are\n",
    "    concatenated and linearly transformed back to the original d_model dimension.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        assert d_model % self.num_heads == 0\n",
    "        self.depth = d_model // self.num_heads\n",
    "\n",
    "        self.wq = torch.nn.Linear(d_model, d_model)\n",
    "        self.wk = torch.nn.Linear(d_model, d_model)\n",
    "        self.wv = torch.nn.Linear(d_model, d_model)\n",
    "        self.dense = torch.nn.Linear(d_model, d_model)\n",
    "\n",
    "    def split_heads(self, x, batch_size):\n",
    "        \"\"\"\n",
    "        Split the last dimension of the input into (num_heads, depth).\n",
    "        Transpose the result such that the shape becomes (batch_size, num_heads, seq_len, depth)\n",
    "        to prepare for parallel computation of attention heads.\n",
    "        \n",
    "        Parameters:\n",
    "        - x (torch.Tensor): The input tensor.\n",
    "        - batch_size (int): Batch size for reshaping.\n",
    "        \n",
    "        Returns:\n",
    "        - torch.Tensor: The reshaped tensor with dimensions (batch_size, num_heads, seq_len, depth).\n",
    "        \"\"\"\n",
    "\n",
    "        x = x.view(batch_size, -1, self.num_heads, self.depth).transpose(1, 2)\n",
    "        return x\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        \"\"\"\n",
    "        The forward pass for the Multi-Head Attention layer.\n",
    "\n",
    "        Parameters:\n",
    "        - q, k, v (torch.Tensor): Queries, keys, and values respectively. These tensors have shapes\n",
    "          (batch_size, seq_len, d_model).\n",
    "        - mask (torch.Tensor, optional): The mask tensor can be used to mask out (ignore) certain positions\n",
    "          during the attention mechanism. This is useful for masking padded positions in the input\n",
    "          sequences or to enforce causality in decoder self-attention. The shape of the mask tensor\n",
    "          should be (batch_size, 1, 1, seq_len) for padding mask, or (batch_size, 1, seq_len, seq_len)\n",
    "          for look-ahead mask.\n",
    "\n",
    "        Returns:\n",
    "        - torch.Tensor: The output after applying multi-head attention. Shape is (batch_size, seq_len, d_model).\n",
    "        - torch.Tensor: The attention weights. Shape is (batch_size, num_heads, seq_len, seq_len).\n",
    "        \"\"\"\n",
    "        \n",
    "        batch_size = q.size(0)\n",
    "\n",
    "        # TODO: Apply linear transformations to the queries, keys, and values\n",
    "        q = None\n",
    "        k = None\n",
    "        v = None\n",
    "        \n",
    "        # TODO: Split the queries, keys, and values into (batch_size, num_heads, seq_len, depth) using the split_heads method\n",
    "        q = None\n",
    "        k = None\n",
    "        v = None\n",
    "\n",
    "        output, attention_weights = scaled_dot_product_attention(q, k, v, mask)\n",
    "\n",
    "        # TODO: Concatenate multiple attention heads\n",
    "        # Hint: The shape of the output should be (batch_size, seq_len, d_model)\n",
    "        output = None\n",
    "\n",
    "        output = self.dense(output)\n",
    "        return output, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Positional Encoding (20 points)\n",
    "\n",
    "Positional Encoding is a method used to inject some information about the relative or absolute position of the tokens in the sequence. The positional encodings have the same dimension as the embeddings so that the two can be summed. There are many choices of positional encodings, learned and fixed. In this assignment, you will implement the fixed positional encoding as described in the following equations:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    PE_{(pos, 2i)} & = \\sin(pos/10000^{2i/d_{model}}), \\\\\n",
    "    PE_{(pos, 2i+1)} & = \\cos(pos/10000^{2i/d_{model}}),\n",
    "\\end{align}\n",
    "$$\n",
    "where $pos$ is the word position and $i$ is the embedding dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(torch.nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "\n",
    "        # TODO: Use the formula given in the original paper to compute the positional encodings\n",
    "        # Hint: Remember to add one more dimension to the encoding tensor so that it can be broadcasted\n",
    "        #   to (batch_size, max_len, d_model) when added to the input tensor\n",
    "        div_term = None\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # TODO: Apply the positional encoding to the input tensor\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Encoder and Decoder (20 points)\n",
    "\n",
    "This section would involve more detailed implementation, including the sub-layer connections, normalization, and how they are combined to form the complete encoder and decoder architecture. Specifically, you will implement the following components:\n",
    "- `EncoderLayer`, which contains a multi-head attention layer and a feed-forward neural network, each followed by a residual connection and layer normalization.\n",
    "- `DecoderLayer`, which contains three sub-layers: masked multi-head attention, multi-head attention, and a feed-forward neural network, each followed by a residual connection and layer normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pointwise_feedforward_network(d_model, dff):\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(d_model, dff),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(dff, d_model)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(torch.nn.Module):\n",
    "    def __init__(self, d_model, num_heads, dff, dropout_rate=0.1):\n",
    "        \"\"\"\n",
    "        Initialize an EncoderLayer.\n",
    "\n",
    "        Parameters:\n",
    "        - d_model (int): The dimensionality of the model.\n",
    "        - num_heads (int): The number of attention heads.\n",
    "        - dff (int): The dimensionality of the feed-forward network model.\n",
    "        - dropout_rate (float): The dropout rate.\n",
    "        \"\"\"\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ffn = pointwise_feedforward_network(d_model, dff)\n",
    "\n",
    "        self.layernorm1 = torch.nn.LayerNorm(d_model)\n",
    "        self.layernorm2 = torch.nn.LayerNorm(d_model)\n",
    "\n",
    "        self.dropout1 = torch.nn.Dropout(dropout_rate)\n",
    "        self.dropout2 = torch.nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        \"\"\"\n",
    "        The forward pass for the EncoderLayer.\n",
    "\n",
    "        Parameters:\n",
    "        - x (Tensor): Input tensor to the encoder layer.\n",
    "        - mask (Tensor, optional): The mask for padding tokens to ignore during self-attention.\n",
    "\n",
    "        Returns:\n",
    "        - Tensor: The output of the encoder layer.\n",
    "        \"\"\"\n",
    "        # TODO: Step 1: Apply multi-head attention (with padding mask) and add & norm\n",
    "        out1 = None\n",
    "\n",
    "        # TODO: Step 2: Apply the feed-forward network and add & norm\n",
    "        out2 = None\n",
    "\n",
    "        return out2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(torch.nn.Module):\n",
    "    def __init__(self, d_model, num_heads, dff, dropout_rate=0.1):\n",
    "        \"\"\"\n",
    "        Initialize a DecoderLayer.\n",
    "\n",
    "        Parameters:\n",
    "        - d_model (int): The dimensionality of the model, i.e., the size of the input and output embeddings.\n",
    "        - num_heads (int): The number of attention heads.\n",
    "        - dff (int): The dimensionality of the feed-forward network model.\n",
    "        - dropout_rate (float): The dropout rate.\n",
    "        \"\"\"\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.mha1 = MultiHeadAttention(d_model, num_heads)  # Self-attention\n",
    "        self.mha2 = MultiHeadAttention(d_model, num_heads)  # Cross-attention\n",
    "        \n",
    "        self.ffn = pointwise_feedforward_network(d_model, dff)\n",
    "        \n",
    "        self.layernorm1 = torch.nn.LayerNorm(d_model)\n",
    "        self.layernorm2 = torch.nn.LayerNorm(d_model)\n",
    "        self.layernorm3 = torch.nn.LayerNorm(d_model)\n",
    "        \n",
    "        self.dropout1 = torch.nn.Dropout(dropout_rate)\n",
    "        self.dropout2 = torch.nn.Dropout(dropout_rate)\n",
    "        self.dropout3 = torch.nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, x, enc_output, look_ahead_mask=None, padding_mask=None):\n",
    "        \"\"\"\n",
    "        The forward pass for the DecoderLayer.\n",
    "\n",
    "        Parameters:\n",
    "        - x (Tensor): Input tensor for decoder layer.\n",
    "        - enc_output (Tensor): Output from the encoder (serves as Key and Value for cross attention).\n",
    "        - look_ahead_mask (Tensor, optional): The mask for future tokens in a sequence within the self-attention mechanism.\n",
    "        - padding_mask (Tensor, optional): The mask for padding tokens within the encoder output.\n",
    "        \"\"\"\n",
    "        # TODO: Step 1: Self attention with look ahead mask and padding mask\n",
    "        out1 = None\n",
    "        \n",
    "        # TODO: Step 2: Cross attention where query comes from previous layer, and key, value come from encoder output\n",
    "        out2 = None\n",
    "        \n",
    "        # TODO: Step 3: Apply the feed forward network\n",
    "        out3 = None\n",
    "        \n",
    "        return out3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(torch.nn.Module):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, max_position_encoding, dropout_rate):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.embedding = torch.nn.Embedding(input_vocab_size, d_model)\n",
    "        self.pos_encoding = PositionalEncoding(d_model, max_position_encoding)\n",
    "        \n",
    "        self.enc_layers = torch.nn.ModuleList([EncoderLayer(d_model, num_heads, dff, dropout_rate) for _ in range(num_layers)])\n",
    "        \n",
    "        self.dropout = torch.nn.Dropout(dropout_rate)\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        # Adding embedding and position encoding\n",
    "        x = self.embedding(x) * math.sqrt(self.d_model)\n",
    "        x = self.pos_encoding(x)\n",
    "        \n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        for i in range(self.num_layers):\n",
    "            x = self.enc_layers[i](x, mask)\n",
    "        \n",
    "        return x  # (batch_size, input_seq_len, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(torch.nn.Module):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size, max_position_encoding, dropout_rate):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.embedding = torch.nn.Embedding(target_vocab_size, d_model)\n",
    "        self.pos_encoding = PositionalEncoding(d_model, max_position_encoding)\n",
    "        \n",
    "        self.dec_layers = torch.nn.ModuleList([DecoderLayer(d_model, num_heads, dff, dropout_rate) for _ in range(num_layers)])\n",
    "        self.dropout = torch.nn.Dropout(dropout_rate)\n",
    "        \n",
    "    def forward(self, x, enc_output, look_ahead_mask, padding_mask):\n",
    "        x = self.embedding(x) * math.sqrt(self.d_model)\n",
    "        x = self.pos_encoding(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        for i in range(self.num_layers):\n",
    "            x = self.dec_layers[i](x, enc_output, look_ahead_mask, padding_mask)\n",
    "\n",
    "        return x  # (batch_size, target_seq_len, d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: Masking (20 points)\n",
    "\n",
    "An important step in training the Transformer model is to mask the attention weights for the future tokens in the sequence.\n",
    "Let's consider a simplified scenario where we have an input sequence in English \"Hello World\" and a hypothetical target sequence in German \"Hallo Welt\" during training. We will tokenize these sequences into numerical tokens (assume a simple tokenization for illustration) and show how the `generate_mask` function generates the padding and look-ahead masks for these sequences.\n",
    "\n",
    "First, let's assign numerical tokens to our sequences. In a real scenario, these would come from a tokenizer's vocabulary:\n",
    "* English (Source) Tokens: \"Hello World\" → [1, 2]\n",
    "* German (Target) Tokens: \"Hallo Welt\" → [1, 2]\n",
    "\n",
    "Assume padding token ID is 0, and both sequences are already padded to a maximum length of 4 for this example:\n",
    "* Padded English Sequence: [1, 2, 0, 0]\n",
    "* Padded German Sequence (with EOS token for simplicity): [1, 2, 3, 0] where 3 is the EOS token.\n",
    "\n",
    "The source mask allows the model to ignore the padding tokens in the source sequence. It would look something like this for the example:\n",
    "```Python\n",
    "src_mask = [[[1, 1, 0, 0]]]  # Shape: (batch_size, 1, 1, src_seq_len)\n",
    "```\n",
    "This indicates that the first two tokens are valid while the last two are padding tokens that should be ignored.\n",
    "\n",
    "The target mask is a combination of padding mask and look-ahead mask to ensure that for predicting each token, the model can only attend to previous tokens and ignores future tokens as well as padding. For our target sequence, considering both padding and look-ahead constraints, the mask might look like:\n",
    "```Python\n",
    "tgt_mask = \n",
    "[[[[1, 0, 0, 0],\n",
    "   [1, 1, 0, 0],\n",
    "   [1, 1, 1, 0],\n",
    "   [1, 1, 1, 0]]]]  # Shape: (batch_size, 1, tgt_seq_len, tgt_seq_len)\n",
    "```\n",
    "Here, the first row allows attention to the first token, the second row to the first and second tokens, and so on. The last token does not attend to future tokens (it can't see them), and since it's an EOS token, it correctly doesn't need to see beyond its position, but the model design might mask it differently based on implementation specifics.\n",
    "\n",
    "Please refer to the comments in the code cells for more detailed instructions on how to implement these two masks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_mask(src, tgt):\n",
    "    \"\"\"\n",
    "    Generates padding and look-ahead masks for source and target sequences.\n",
    "    Suppose that the padding token is 0.\n",
    "\n",
    "    Parameters:\n",
    "    - src (Tensor): The source sequence tensor with shape (batch_size, src_seq_len).\n",
    "    - tgt (Tensor): The target sequence tensor with shape (batch_size, tgt_seq_len).\n",
    "\n",
    "    Returns:\n",
    "    - Tensor: The padding mask for the source sequence.\n",
    "    - Tensor: The combined padding and look-ahead mask for the target sequence.\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO: Create a mask for the source sequence padding tokens.\n",
    "    # This mask is used to ignore the padding tokens in the source sequence during the attention calculations.\n",
    "    # The mask has dimensions (batch_size, 1, 1, src_seq_len) after unsqueezing, suitable for broadcasting\n",
    "    # with the attention scores tensor.\n",
    "    src_mask = None\n",
    "\n",
    "    # TODO: Create a mask for the target sequence padding tokens.\n",
    "    # Similar to src_mask, but for the target sequence, preparing it for broadcasting.\n",
    "    # The initial dimensions after unsqueezing are (batch_size, 1, tgt_seq_len, 1), which is suitable for\n",
    "    # look-ahead masking when combined with the nopeak_mask.\n",
    "    tgt_mask = None\n",
    "\n",
    "    # TODO: Generate a no-peek (look-ahead) mask to prevent positions from attending to subsequent positions.\n",
    "    # This is crucial for the target sequence during training, ensuring predictions for position i\n",
    "    # can only depend on the known outputs at positions less than i.\n",
    "    nopeak_mask = None\n",
    "\n",
    "    # TODO: Combine the padding mask and the look-ahead mask for the target sequence.\n",
    "    # This ensures that the model does not attend to padding tokens and future tokens.\n",
    "    tgt_mask = None\n",
    "\n",
    "    return src_mask, tgt_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Putting all these components together, we get our complete Transformer model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(torch.nn.Module):\n",
    "    def __init__(self, num_encoder_layers, num_decoder_layers, d_model, num_heads, dff, input_vocab_size, target_vocab_size, pe_input, pe_target, dropout_rate=0.1):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.encoder = Encoder(num_encoder_layers, d_model, num_heads, dff, input_vocab_size, pe_input, dropout_rate)\n",
    "        self.decoder = Decoder(num_decoder_layers, d_model, num_heads, dff, target_vocab_size, pe_target, dropout_rate)\n",
    "        self.final_layer = torch.nn.Linear(d_model, target_vocab_size)\n",
    "\n",
    "    def forward(self, inp, tar):\n",
    "        # Generate masks\n",
    "        src_mask, tgt_mask = generate_mask(inp, tar)\n",
    "\n",
    "        # Pass the input through the encoder, which uses src_mask\n",
    "        enc_output = self.encoder(inp, src_mask)  # (batch_size, inp_seq_len, d_model)\n",
    "\n",
    "        # Pass the encoder output and target through the decoder, which uses tgt_mask and src_mask\n",
    "        dec_output = self.decoder(tar, enc_output, tgt_mask, src_mask)  # (batch_size, tar_seq_len, d_model)\n",
    "\n",
    "        final_output = self.final_layer(dec_output)  # (batch_size, tar_seq_len, target_vocab_size)\n",
    "\n",
    "        return final_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you move on to experiment on a real dataset, you may verify that your Transformer model is working correctly with a synthetically generated dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_vocab_size = 5000\n",
    "tgt_vocab_size = 5000\n",
    "d_model = 512\n",
    "num_heads = 8\n",
    "num_layers = 2\n",
    "d_ff = 2048\n",
    "max_seq_length = 100\n",
    "dropout = 0.1\n",
    "\n",
    "transformer = Transformer(\n",
    "    num_layers, num_layers, d_model, num_heads, d_ff,\n",
    "    src_vocab_size, tgt_vocab_size,\n",
    "    pe_input=max_seq_length, pe_target=max_seq_length, dropout_rate=dropout).to(device)\n",
    "\n",
    "src_data = torch.randint(1, src_vocab_size, (64, max_seq_length)).to(device)  # (batch_size, seq_length)\n",
    "tgt_data = torch.randint(1, tgt_vocab_size, (64, max_seq_length)).to(device)  # (batch_size, seq_length)\n",
    "\n",
    "output = transformer(src_data, tgt_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5: Model Training and Evaluation (no grading)\n",
    "\n",
    "In this section, you will train your Transformer model on a translation task using the WMT German-English dataset.\n",
    "After training, you will evaluate the model on a test set using BLEU score as the evaluation metric.\n",
    "Due to the large size of the WMT dataset, we will not be grading this part. However, you are encouraged to experiment with the real dataset to see how well your model performs.\n",
    "Before you start training, make sure that you have installed the necessary packages and have access to a GPU for faster training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install datasets transformers sacrebleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset('wmt14', 'de-en', split={'train': 'train[:1%]', 'test': 'test', 'validation': 'validation'})\n",
    "train_data = dataset['test']\n",
    "valid_data = dataset['validation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('facebook/bart-base')\n",
    "\n",
    "def tokenize_data(examples, max_length=128):\n",
    "    inputs = [ex['de'] for ex in examples['translation']]\n",
    "    targets = [ex['en'] for ex in examples['translation']]\n",
    "    model_inputs = tokenizer(inputs, max_length=max_length, truncation=True, padding='max_length')\n",
    "    labels = tokenizer(targets, max_length=max_length, truncation=True, padding='max_length')\n",
    "\n",
    "    model_inputs['labels'] = labels['input_ids']\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_train_data = train_data.map(tokenize_data, batched=True).with_format(type='torch', columns=['input_ids', 'labels'])\n",
    "tokenized_valid_data = valid_data.map(tokenize_data, batched=True).with_format(type='torch', columns=['input_ids', 'labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(tokenized_train_data, batch_size=32, shuffle=True)\n",
    "valid_loader = DataLoader(tokenized_valid_data, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To determine the appropriate length of the maximum sequence, you might want to inspect the length distribution of the training data. You can then set the maximum sequence length to a value that covers most of the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "lengths = [len(tokenizer.tokenize(example['en'])) for example in train_data['translation']]\n",
    "\n",
    "lengths = np.array(lengths)\n",
    "mean_length = np.mean(lengths)\n",
    "max_length = np.max(lengths)\n",
    "median_length = np.median(lengths)\n",
    "percentile_90 = np.percentile(lengths, 90)\n",
    "\n",
    "print(f\"Mean length: {mean_length}\")\n",
    "print(f\"Max length: {max_length}\")\n",
    "print(f\"Median length: {median_length}\")\n",
    "print(f\"90th percentile length: {percentile_90}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "num_encoder_layers = 6\n",
    "num_decoder_layers = 6\n",
    "max_seq_length = 512\n",
    "d_model = 512\n",
    "num_heads = 8\n",
    "dff = 2048\n",
    "dropout_rate = 0.1\n",
    "input_vocab_size = tokenizer.vocab_size\n",
    "target_vocab_size = tokenizer.vocab_size\n",
    "\n",
    "model = Transformer(\n",
    "    num_layers, num_layers, d_model, num_heads, d_ff,\n",
    "    input_vocab_size, target_vocab_size,\n",
    "    pe_input=max_seq_length, pe_target=max_seq_length, dropout_rate=dropout).to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0005)\n",
    "criterion = torch.nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In sequence-to-sequence (seq2seq) models, such as Transformers, the label shifting technique, often referred to as \"teacher forcing\" when used during training, plays a crucial role in the learning process. This technique involves shifting the labels by one position so that the model predicts the next token in the sequence given all the previous tokens up to that point.\n",
    "\n",
    "Suppose we have the following English sentence (source) and its French translation (target):\n",
    "- **English (Source)**: \"Hello, world\"\n",
    "- **French (Target)**: \"Bonjour, le monde\"\n",
    "\n",
    "Input to the Model (Encoder Input): The input sequence to the encoder would be the English sentence, tokenized and possibly including special tokens like start-of-sequence (SOS) or end-of-sequence (EOS) tokens, depending on the model architecture:\n",
    "- **Encoder Input**: `[SOS] Hello, world [EOS]`\n",
    "\n",
    "Target Sequence for Teacher Forcing (Decoder Input): The target sequence for teacher forcing (used as input to the decoder) is shifted by one token to the right, to teach the model to predict the next token in the sequence. It includes an SOS token at the beginning to indicate the start of the sequence but omits the EOS token or includes it only as part of the ground truth for the final step, ensuring that for each input token, the model learns to predict the subsequent token:\n",
    "- **Decoder Input**: `[SOS] Bonjour, le monde`\n",
    "\n",
    "The ground truth data against which the model's predictions are compared is the target sequence shifted one position to the left, excluding the SOS token and including the EOS token. This ensures that for every step of the sequence, the model is trained to predict the next token:\n",
    "- **Ground Truth Data**: `Bonjour, le monde [EOS]`\n",
    "\n",
    "To visualize the shifting, consider how each token in the decoder input is used to predict the corresponding token in the ground truth data:\n",
    "- Decoder Input: `[SOS]` → Predicts → `Bonjour`\n",
    "- Decoder Input: `Bonjour` → Predicts → `,`\n",
    "- Decoder Input: `,` → Predicts → `le`\n",
    "- Decoder Input: `le` → Predicts → `monde`\n",
    "- Decoder Input: `monde` → Predicts → `[EOS]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "for epoch in range(50):\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        output = model(input_ids, labels[:, :-1])\n",
    "        output_dim = output.shape[-1]  # Vocabulary size\n",
    "        # Reshape output to (batch_size * seq_len, output_dim) for calculating loss\n",
    "        output = output.reshape(-1, output_dim)\n",
    "        labels = labels[:, 1:].reshape(-1)  # Flatten labels to align with output for CrossEntropyLoss\n",
    "\n",
    "        loss = criterion(output, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hugging Face provides various metrics through its datasets library. For translation tasks, BLEU is a common metric used to evaluate the quality of the model's translations. You can use the `datasets` library to load the WMT dataset and evaluate your model using BLEU score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "\n",
    "bleu_metric = load_metric('sacrebleu')\n",
    "\n",
    "model.eval()\n",
    "predictions = []\n",
    "references = []\n",
    "\n",
    "for batch in valid_loader:\n",
    "    input_ids = batch['input_ids'].to(device)\n",
    "    labels = batch['labels'].to(device)  # Ground truth labels\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, labels)\n",
    "    \n",
    "    # Convert model outputs to predicted tokens\n",
    "    predicted_tokens = torch.argmax(outputs, dim=-1)\n",
    "    \n",
    "    # Convert tokens to texts\n",
    "    predicted_texts = [tokenizer.decode(ids, skip_special_tokens=True) for ids in predicted_tokens]\n",
    "    reference_texts = [[tokenizer.decode(ids, skip_special_tokens=True)] for ids in labels]\n",
    "    \n",
    "    predictions.extend(predicted_texts)\n",
    "    references.extend(reference_texts)\n",
    "\n",
    "# Compute BLEU score\n",
    "results = bleu_metric.compute(predictions=predictions, references=references)\n",
    "print(f\"BLEU score: {results['score']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_idx = np.random.randint(0, len(predictions))\n",
    "print(f\"Reference: {references[random_idx][0]}\")\n",
    "print(f\"Predicted: {predictions[random_idx]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might notice that there are lot of repetitions of the word \"the\" in the decoder's output.\n",
    "This is a common issue known as \"repetition problem\" or \"degeneration problem.\" This usually occurs during the generation phase, where the model falls into a loop, outputting the same word or phrase repeatedly. This is mostly due to the inadequate training of the model. There are several techniques to mitigate this issue, such as using beam search, nucleus sampling, or top-k sampling during the generation phase."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
