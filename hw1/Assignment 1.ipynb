{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS247 Advanced Data Mining - Assignment 1\n",
    "## Deadline: 11:59PM, January 24, 2023\n",
    "\n",
    "## Instructions\n",
    "Each assignment is structured as a Jupyter notebook, offering interactive tutorials that align with our lectures. You will encounter two types of problems: *write-up problems* and *coding problems*.\n",
    "\n",
    "1. **Write-up Problems:** These problems are primarily theoretical, requiring you to demonstrate your understanding of lecture concepts and to provide mathematical proofs for key theorems. Your answers should include sufficient steps for the mathematical derivations.\n",
    "2. **Coding Problems:** Here, you will be engaging with practical coding tasks. These may involve completing code segments provided in the notebooks or developing models from scratch.\n",
    "\n",
    "To ensure clarity and consistency in your submissions, please adhere to the following guidelines:\n",
    "\n",
    "* For write-up problems, use Markdown bullet points to format text answers. Also, express all mathematical equations using $\\LaTeX$ and avoid plain text such as `x0`, `x^1`, or `R x Q` for equations.\n",
    "* For coding problems, comment on your code thoroughly for readability and ensure your code is executable. Non-runnable code may lead to a loss of **all** points. Coding problems have automated grading, and altering the grading code will result in a deduction of **all** points.\n",
    "* Your submission should show the entire process of data loading, preprocessing, model implementation, training, and result analysis. This can be achieved through a mix of explanatory text cells, inline comments, intermediate result displays, and experimental visualizations.\n",
    "\n",
    "### Submission Requirements\n",
    "\n",
    "* Submit your solutions through GradeScope in BruinLearn.\n",
    "* Late submissions are allowed up to 24 hours post-deadline with a penalty factor of $\\mathbf{1}(t\\leq24)e^{-(\\ln(2)/12)t}$.\n",
    "\n",
    "### Collaboration and Integrity\n",
    "\n",
    "* Collaboration is encouraged, but all final submissions must be your own work. Please acknowledge any collaboration or external sources used, including websites, papers, and GitHub repositories.\n",
    "* Any suspicious cases of academic misconduct will be reported to The Office of the Dean of Students.\n",
    "\n",
    "## Outline\n",
    "* Problem 1: Naïve Bayes (50 points)\n",
    "* Problem 2: Logistic Regression (50 points)\n",
    "* Problem 3: Gaussian Mixture Models (70 points + 10 bonus points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1: Naïve Bayes (50 points) <a class=\"anchor\" id=\"problem-1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.1: MLE Estimation of Model Parameters (10 points)\n",
    "\n",
    "Recall that given a corpus and labels for each document $D = \\{(\\boldsymbol{x}_d, y_d)\\}_{d=1}^{|D|}$, the log likelihood function of a Bayes model parameterized by $\\Theta = (\\boldsymbol\\beta_1, \\boldsymbol\\beta_2, \\boldsymbol\\beta_3, \\boldsymbol\\pi)$ is:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathcal{L}(\\Theta) & = \\log \\prod_{d=1}^{|D|} p(\\boldsymbol{x}_d, y_d | \\Theta) \\\\\n",
    "& = \\sum_{d=1}^{|D|} \\log p(\\boldsymbol{x}_d, y_d | \\Theta) \\\\\n",
    "& = \\sum_{d=1}^{|D|} \\log [p(\\boldsymbol{x}_d | y_d, \\Theta) p(y_d | \\Theta)] \\\\\n",
    "& = \\sum_{d=1}^{|D|} \\left(\\sum_{n=1}^{N}x_{dn}\\log\\beta_{y_d,n} + \\log \\pi_{y_d}\\right).\n",
    "\\end{align}\n",
    "$$\n",
    "Show the MLE estimator of $\\boldsymbol\\beta$. Include necessary derivatives in your answer. (Hint: use Lagrange multipliers to enforce the constraint $\\sum_{n=1}^{N}\\beta_{j,n} = 1$ for all $j$.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[TODO: Write your answer here]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execrise 1.2: Implementation of Naïve Bayes in scikit-learn (10 points)\n",
    "\n",
    "In this exercise, you will learn the basics of using scikit-learn to implement a Naïve Bayes classifier. We will use the [Sentiment Polarity Dataset Version 2.0](https://www.cs.cornell.edu/people/pabo/movie-review-data/) for this exercise. This dataset contains 1000 positive and 1000 negative reviews. We have provided the code for dataset preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and load the dataset\n",
    "\n",
    "import nltk\n",
    "from sklearn.datasets import load_files\n",
    "\n",
    "nltk.download('movie_reviews', download_dir='.', quiet=True)\n",
    "reviews = load_files('./corpora/movie_reviews', shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment analysis: `neg` for negative sentiment, `pos` for positive sentiment\n",
    "reviews.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of reviews\n",
    "len(reviews.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training and test sets\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "reviews_train, reviews_test, y_train, y_test = train_test_split(\n",
    "    reviews.data, reviews.target, test_size=0.20, random_state=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset preprocessing: convert reviews to a matrix of token counts\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count_vect = CountVectorizer().fit(reviews.data) \n",
    "X_train = count_vect.transform(reviews_train).toarray()\n",
    "X_test = count_vect.transform(reviews_test).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 1.2.1 (8 points)\n",
    "Train the multinomial Naïve Bayes model on the training set and test it on the test set. Report the accuracy of the model on the test set.\n",
    "You would be able to achieve accuracy of around 80% on the test set.\n",
    "\n",
    "(Hint: Refer to the [scikit-learn documentation](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html) for more details on how to use the `MultinomialNB` class.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and test with MultinomialNB\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "clf = MultinomialNB()\n",
    "\n",
    "# TODO: Use the `MultinomialNB` class to train the model on the training set and test it on the test set\n",
    "# TODO: Report the accuracy of the model on the test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 1.2.2 (2 points)\n",
    "There are two functions of the Naïve Bayes classifier in order to get the probabilities of the predictions: `predict_proba` and `predict_log_proba`. What is the difference between them? What is the advantage of using `predict_log_proba` over `predict_proba`?\n",
    "\n",
    "(Hint: If you are unsure about this question, please proceed to Exercise 1.3.1 and come back to this question later.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[TODO: Write your answer here]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.3: Your Implementation of Naïve Bayes (30 points)\n",
    "\n",
    "In this exercise, you will implement a Naïve Bayes classifier by yourself and compare it with the scikit-learn implementation. You will use the same dataset as in Exercise 1.2.\n",
    "We have provided a code skeleton for your implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 1.3.1 (24 points)\n",
    "Implement the `fit`, `predict_proba`, and `predict_log_proba` methods in the `NaiveBayes` class, according to what we have learned from lectures.\n",
    "Each implementation is worth 8 points.\n",
    "Please try to optimize for efficiency. Your code should run in seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayes(object):\n",
    "    \"\"\"\n",
    "    Your implementation of Naive Bayes classifier.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, alpha=1.0):\n",
    "        \"\"\"\n",
    "        Initialize the Naive Bayes classifier.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        alpha : float, default=1.0\n",
    "            Additive (Laplace/Lidstone) smoothing parameter\n",
    "            (0 for no smoothing).\n",
    "        \"\"\"\n",
    "\n",
    "        self.alpha = alpha\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fit the Naive Bayes classifier on the training set (X, y).\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape (n_samples, n_features)\n",
    "            Training vectors, where n_samples is the number of samples\n",
    "            and n_features is the number of features.\n",
    "        y : array-like, shape (n_samples,)\n",
    "            Target values.\n",
    "        \"\"\"\n",
    "\n",
    "        self.n_features = X.shape[1]\n",
    "        self.n_samples = X.shape[0]\n",
    "        self.n_classes = np.unique(y).shape[0]\n",
    "\n",
    "        self.beta = np.zeros((self.n_classes, self.n_features))\n",
    "        self.pi = np.zeros(self.n_classes)\n",
    "\n",
    "        # TODO: Given (X, y), compute the parameters `beta` and `pi`\n",
    "        # (Hint: Calculate `beta` according to the frequencies of words\n",
    "        #    and `pi` according to the class frequencies.\n",
    "        #    Remember to consider `alpha` for the Laplace smoothing.)\n",
    "\n",
    "        self.log_beta = np.log(self.beta)\n",
    "        self.log_pi = np.log(self.pi)\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "        Return posterior probabilities of classification for X.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape (n_samples, n_features)\n",
    "            Test vectors, where n_samples is the number of samples\n",
    "            and n_features is the number of features.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        y_prob : array-like, shape (n_samples, n_classes)\n",
    "            Posterior probabilities of classification per class.\n",
    "        \"\"\"\n",
    "\n",
    "        # TODO: Given `X``, return the posterior probabilities of classification\n",
    "        # (Hint: Use `beta`` and `pi`. Remember to normalize the probabilities.)\n",
    "        \n",
    "        pass\n",
    "\n",
    "    def predict_log_proba(self, X):\n",
    "        \"\"\"\n",
    "        Return posterior log probabilities of classification for X.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape (n_samples, n_features)\n",
    "            Test vectors, where n_samples is the number of samples\n",
    "            and n_features is the number of features.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        y_log_prob : array-like, shape (n_samples, n_classes)\n",
    "            Posterior log probabilities of classification per class.\n",
    "        \"\"\"\n",
    "\n",
    "        # TODO: Given X, return the posterior log probabilities of classification\n",
    "        # (Hint: Use `log_beta`` and `log_pi`.)\n",
    "\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_clf = NaiveBayes()\n",
    "my_clf.fit(X_train, y_train)\n",
    "\n",
    "# Sanity checks\n",
    "assert my_clf.n_features == X_train.shape[1]\n",
    "assert my_clf.n_samples == X_train.shape[0]\n",
    "assert my_clf.n_classes == np.unique(y_train).shape[0]\n",
    "assert my_clf.beta.shape == (my_clf.n_classes, my_clf.n_features)\n",
    "assert my_clf.pi.shape == (my_clf.n_classes,)\n",
    "assert np.isclose(my_clf.pi.sum(), 1)\n",
    "assert np.allclose(my_clf.beta.sum(axis=1), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 1.3.2 (6 points)\n",
    "\n",
    "Compare the performance of your implementation with sklearn's implementation on the test set.\n",
    "Report the accuracy of both implementations on the test set.\n",
    "Ideally, you should be able to achieve accuracy of around 80% on the test set, similar to the scikit-learn implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Report the accuracy of your implementation on the test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2: Logistic Regression (50 points)\n",
    "\n",
    "In this problem, we review the classification model known as logistic regression. This model is motivated from a probabilistic perspective, hence we start with this formulation. In the following, we assume that:\n",
    "- Data (boldfont, uppercase X) $\\mathbf{X} \\in \\mathbb{R}^{n\\times d}$, $n$ is the number of data points and $d$ the number of features.\n",
    "- $\\mathbf{x}_i$ is a single data point and $\\mathbf{x}_i \\in \\mathbb{R}^d$.\n",
    "- $\\mathbf{Y} \\in \\mathbb{R}^n$ are the labels of the data points, $Y_i$ is the label of $\\mathbf{x}_i$.\n",
    "- We denote (uppercase letter) $X,Y,Z$,... to be __random variables__, which are measurable maps from the sample space to the real line: $X: \\Omega \\rightarrow \\mathbb{R}$\n",
    "- We denote $P_X$ to be the __probability distribution__ associated with the random variable $X$.\n",
    "- We denote $f_X$ to be the __probability density function (pdf)__ associated with the random variable $X$, if it exists. If the random variable is discrete, then we also use $P_X$ to denote its __probability mass function (pmf)__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.1: Maximum Likelihood Formulation (12 points)\n",
    "\n",
    "Maximum Likelihood Estimation (MLE) is the most standard probabilistic learning framework. In this formulation, we:\n",
    "- Assume that __conditional distribution of label given data__, $P(\\mathbf{Y}\\mid\\mathbf{X})$, follows a parametric probabilistic distribution.\n",
    "- Assume that data pairs are sampled __independently and identically (i.i.d)__ from the distribution. This means $P_{Y\\mid X} := P(\\mathbf{Y}\\mid\\mathbf{X}) = \\prod_{i=1}^n P(\\mathbf{y}_i\\mid\\mathbf{x}_i)$.\n",
    "\n",
    "__As a technical note__, here $P$ is the probability distribution for the random variable $Y\\mid\\mathbf{X}$, not the _probability density function (pdf)_; however, for well-behaved distributions, such as those from the __exponential family__ (including Gaussian, Bernoulli, Poisson, Exponential, etc.), we can just use their pdfs in the case of independence. This means if i.i.d assumption holds, then the joint density function factorizes:\n",
    "$$\n",
    "f_{X}(x_1,x_2,\\cdots, x_n) = f_{X_1,X_2,\\cdots, X_n}(x_1,x_2,\\cdots, x_n) = \\prod_{i=1}^n f_{X_i}(x_i)\n",
    "$$\n",
    "\n",
    "__Example__: In the case of linear regression, we have that $P_{y_i\\mid\\mathbf{X}_i} \\sim \\mathcal{N}(y_i; W^\\top \\mathbf{X}_i + b_i, \\Sigma)$. In this case we have\n",
    "$$\n",
    "f_{Y\\mid X}(x_1, x_2, \\cdots, x_n, y_1, \\cdots, y_n) = \\prod_{i=1}^n f_{Y_i\\mid X_i}(x_i) = \\prod_{i=1}^n \\frac{1}{(2\\pi)^{d/2}|\\Sigma|^{1/2}}\\exp \\left\\{-\\frac{1}{2} (y_i - W^\\top \\mathbf{x}_i - b_i)^\\top \\Sigma^{-1} (y_i - W^\\top \\mathbf{x}_i-b_i) \\right\\}\n",
    "$$\n",
    "\n",
    "To perform a MLE inference, we need to obtain the __likelihood function__, which for exponential family distributions, can simply be written as the joint density function above, but now we change the notation since for likelihood function, __parameters of the distributions are treated as inputs__, e.g.:\n",
    "$$\n",
    "L(W, b, \\Sigma) = \\prod_{i=1}^n \\frac{1}{(2\\pi)^{d/2}|\\Sigma|^{1/2}}\\exp \\left\\{-\\frac{1}{2} (y_i - W^\\top \\mathbf{x}_i - b_i)^\\top \\Sigma^{-1} (y_i - W^\\top \\mathbf{x}_i-b_i) \\right\\}\n",
    "$$\n",
    "\n",
    "However, product term is tricky to deal with, we hence need __log-likelihood__ function. $l(W,b,\\Sigma) = \\log L(W,b,\\Sigma)$ to turn the product into summation. From their on, we can invoke any optimization algorthm (or do it by hand), to maximize the log-likelihood function.\n",
    "\n",
    "Now, the exercise is to write down the formulation of MLE for the following two classes of distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 2.1.1 (3 points)\n",
    "\n",
    "Write down the log-likelihood function for $n$ i.i.d random samples from the Bernoulli distribution, which has distribution function\n",
    "$$P_X(x_i;p) = p^{x_i}(1-p)^{(1-x_i)}$$\n",
    "where $p\\in [0,1]$ and $x_i \\in \\{0,1\\}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[TODO: Write your answer here]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 2.1.2 (3 points)\n",
    "\n",
    "Write down the log-likelihood function for $n$ i.i.d random samples from the Poisson distribution, which has distribution function\n",
    "$$P_X(x_i; \\lambda) = \\frac{\\lambda^{x_i} e^{-\\lambda}}{{x_i}!}$$\n",
    "where $x_i \\in \\{1,2,\\cdots,\\}$.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[TODO: Write your answer here]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 2.1.3 (3 points)\n",
    "\n",
    "Write down the __negative log-likelihood__ function for the logistic regression model applied to $n$ data points, where for each observed pair $(\\mathbf{x}_i, y_i)$, we assume that $P_{y_i\\mid X_i} \\sim \\text{Bernoulli}(\\sigma(W^\\top \\mathbf{x}_i))$.\n",
    "\n",
    "(Hint: Refer to Exercise 2.1.1 for the parametric form. Also you don't need to write down exact formula for the sigmoid function $\\sigma$.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[TODO: Write your answer here]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 2.1.4 (3 points)\n",
    "\n",
    "Suppose you want to perform maximum likelihood inference for Gaussian, Poisson, and Bernoulli random variables. What would be the difference, in terms of optimization procedure, when you deal with these three random variables?\n",
    "\n",
    "(Hint: Think about the values the parameters of these distributions can take.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[TODO: Write your answer here]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.2: Implemention of Logistic Regression (38 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.optimize as opt\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import metrics\n",
    "from scipy.stats import multivariate_normal\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler, normalize\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 2.2.1 (8 points)\n",
    "\n",
    "Analytically calculate the gradient of logistic regression negative log-likelihood with respect to parameter $W$. Show all of your steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[TODO: Write your answer here]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following exercises, we load a sample dataset, and then implement a logistic regression model by hand; in the end, we compare the prediction performance with a standard logistic regression model from the scikit-learn library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data = pd.read_csv(\"sample_data.txt\", delimiter=\",\")\n",
    "n,d = sample_data.shape\n",
    "sample_data.columns = [\"score_1\", \"score_2\", \"label\"]\n",
    "X = sample_data[[\"score_1\", \"score_2\"]].values\n",
    "y = sample_data[\"label\"].values.reshape(-1,1)\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1234)\n",
    "W_init = np.random.randn(d,1)\n",
    "W_init.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 2.2.2 (10 points)\n",
    "Implement a logistic regression model by hand, by specifying the following functions:\n",
    "- `sigmoid(x)` which implements the sigmoid function\n",
    "- `cost_function(W, X, y)`, which returns the negative log-likelihood and the gradient with respect to parameter `W`\n",
    "\n",
    "(Hint: Tou may want to deal with numerical issues with `log` with something like `eps=1e-6`.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    # TODO: Implement the sigmoid function\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# W: Model parameters of shape (d,1)\n",
    "# X: Input of shape (N,d)\n",
    "# y: Input of shape (N,1)\n",
    "# Returns: Negative-log-likelihood, gradient\n",
    "def cost_function(W, X, y):\n",
    "    # TODO: Implement the negative log-likelihood function and its gradient with respect to parameter `W`\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 2.2.3 (6 points)\n",
    "\n",
    "Implement gradient descent and use it to train the logistic regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Gradient descent algorithm: update the parameter `W` at each iteration\n",
    "Returns: \n",
    "    - A `List` recording losses at each iteration\n",
    "    - The final optimal value of `W`\n",
    "\"\"\"\n",
    "def gradient_descent(X, y, W, learning_rate, n_iters):\n",
    "    # TODO: Implement the gradient descent algorithm\n",
    "    return [], W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Run gradient descent and obtain the records alongside optimal `W` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 2.2.4 (6 points)\n",
    "\n",
    "Plot the training curve (y-axis as the negative log-likelihood, and x-axis as training iterations), and experiment on different learning rates and `n_iters` to ensure convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Use matplotlib to plot the losses and show convergence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 2.2.5 (8 points)\n",
    "\n",
    "Use the `sklearn.linear_model.LogisticRegression` model to fit the given `X` and `y`, then report the optimum values obtained from this model and compare with the result from Exercise 2.2.3.\n",
    "Are the optimal value of parameters similar? If so, explain why. If not, explain what may cause the difference between your implementation and the one from sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: scikit-learn logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[TODO: Write your answer here]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3: Gaussian Mixture Model (70 points + 10 bonus points)\n",
    "\n",
    "In this problem, we will review a commonly used __latent variable model__ called Gaussian Mixture Model (GMM). In particular, we will focus on the property of learning this model, first from the perspective of MLE and then from the perspective of surrogate optimization, using what is known as the __Expectation Maximization (EM)__ algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.1: Full Distribution of GMM (6 points)\n",
    "\n",
    "We know that for _generative modeling_, the dataset is assumed to be generated from some probabilistic distribution, and the goal of GMM is to estimate the underlying distribution of the dataset. Here the assumption is implicitly that the data is generated by a probability distribution smooth enough, such that we can estimate its probability density function (pdf). Hence the terminology for this task is known as __density estimation__. A straightforward application of density estimation is the task of __clustering__, e.g., determining the possible __modes__ of the probability density function (each cluster center can be seen as the mode of a pdf). Therefore, from this perspective, we can make the following observations about K-Means and GMM:\n",
    "\n",
    "- K-Means is an algorithm that targets the task of clustering specifically, while GMM is more powerful in that it deals with density estimation, and by this virtue it can also be used for clustering.\n",
    "- In the context of clustering, GMM assigns data points _softly_, that is, with some probability, to a cluster, whereas K-means provides _hard assignments_ of elements to clusters.\n",
    "\n",
    "Suppose that the dataset contains $n$ elements $\\{x_1, \\cdots, x_N\\}$ and we assume $k$ cluster centers (or distribution modes). Now we take a look at the generative modeling story that GMM is telling: as a latent variable model, each observed data point $\\mathbf{x}_i$ is associated with two random variables $Z_i, X_i$, such that $P_{X_i,Z_i} = P_{Z_i}P_{X_i\\mid Z_i}$ where:\n",
    "\n",
    "\n",
    "- $Z_i \\sim Cat(\\pi_1, \\cdots, \\pi_k)$, where $k$ represents $k$ pdf modes (clusters) and $\\sum_{j=1}^k \\pi_j = 1$; this random variable is quantifies the event \\{$\\mathbf{x}_i$ belongs to cluster $j$\\}, and $P(Z_j = i) = \\pi_i$.\n",
    "\n",
    "Now comes the trickier bit, where we specify a __conditional distribution__ $X_i\\mid Z_i=j$. This means __after observing that $\\mathbf{x}_i$ is in cluster $j$, what is the distribution of $x_i$?__. The GMM assumes this follows a Gaussian with mean and variance determined by the clsuter $j$:\n",
    "\n",
    "- $X_i\\mid Z_i = j \\sim \\mathcal{N}(\\mu_j, \\Sigma_j)$, where the index $j \\in \\{1,\\cdots, N\\}$ and index $j\\in\\{1,\\cdots, k\\}$. We can see that each random variable $X_i\\mid Z_i=j$ is distributed as a Gaussian, whose parameters are determined by the cluster it belongs to.\n",
    "\n",
    "The plate diagram for this _Probabilistic Graphical Model_ is given below, where the empty circle represents the latent variable (since we cannot observe the clusters a priori).\n",
    "\n",
    "![](bishop-gaussian-mixture.png)\n",
    "\n",
    "This plate diagram tells us that there are in total $2N$ random variables, two for each observed data point. It also allows us to write down the joint density function, where again we have the i.i.d assumption of each random variable pair $(X_i,Z_i)$ and we denote the full joint distribution as $P_{XZ}$ (and the full joint pdf as $f_{XZ}$) and the individual joint distribution to be $P_{X_iZ_i}$(and the individual joint pdf to be $f_{X_iZ_i}$):\n",
    "\n",
    "$$\n",
    "f_{X_iZ_i}(\\mathbf{\\mathbf{x}_i}, k) = P(Z_i = k) f_{X}(\\mathbf{x_i} \\mid \\mu_k, \\Sigma_k) = \\pi_k \\mathcal{N}(\\mathbf{x}; \\mu_k, \\Sigma_k)\n",
    "$$\n",
    "\n",
    "\n",
    "__Please strictly follow the notation used in this notebook, as your true understanding of the model should not change with a change of notation__!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 3.1.1 (3 points)\n",
    "\n",
    "Write down the full derivation of the joint density function $f_{XZ}$ from the formula above.\n",
    "\n",
    "(Hint: Write down the correct pdf for each random variable, then use the assumption of i.i.d to factorize it. Note that $\\mathbf{x}_i \\in \\mathbb{R}^d$.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[TODO: Write your answer here]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 3.1.2 (3 points)\n",
    "\n",
    "Write down the formula for the marginal joint pdf of $f_X$ using the result from Exercise 3.1.1, where $X$ is the collection of random variables $X_1,\\cdots, X_N$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[TODO: Write your answer here]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.2: MLE and EM algorithm for GMM (9 points + 10 bonus points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 3.2.1 (5 points)\n",
    "\n",
    "After obtaining the joint PDF, we now can derive the negative log-likelihood function of the joint pdf. write down the formula for the negative log-likelihood for random variable $X = (X_1,.\\cdots, X_N)$. Then answer the following: why is doing a vanilla MLE for GMM difficult?\n",
    "\n",
    "(Hint: It is okay to Google this, but you need to write down the analytical form of the negative log-likelihood and then say something about it.\n",
    "You may want to read Section 9.2.1 from Christopher Bishop's [_Pattern Recognition and Machine Leanrning_](https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf).)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[TODO: Write your answer here]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 3.2.2 (4 points)\n",
    "\n",
    "Write down the EM algorithm for GMM (i.e. write down the 4 steps)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[TODO: Write your answer here]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 3.2.3 (10 bonus points)\n",
    "Prove that the EM algorithm guarantees monotonic increase in the log-likelihood using the following theorem (or any version of it, by providing your source citation).\n",
    "\n",
    "__Theorem (Jensen's Inequality)__: If $f:\\mathbb{R}\\rightarrow \\mathbb{R}$ is a concave function, then for any $x_1,\\cdots, x_k$, and any $\\lambda_1,\\cdots, \\lambda_k \\geq 0$, and $\\sum_{i=1}^k \\lambda_k = 1$, the following inequality holds:\n",
    "$$\n",
    "\\sum_{j=1}^m \\lambda_j f(a_j) \\leq f\\left( \\sum_{j=1}^m \\lambda_j a_j \\right)\n",
    "$$\n",
    "\n",
    "__Hints__:\n",
    "- Is the logarithm function a concave function?\n",
    "- You may want to checkout the general case of EM algorithm monotonicity [here](https://www.cs.cmu.edu/~epxing/Class/10708-17/notes-17/10708-scribe-lecture8.pdf). Think about the special case of GMM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[TODO: Write your answer here]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.3: EM vs. GD on Convergence (55 points)\n",
    "\n",
    "In this numerical experiment, we study and compare two cases:\n",
    "1. GMM trained using MLE, with gradient descent;\n",
    "2. GMM trained using EM algorithm.\n",
    "\n",
    "We compare these two scenarios' convergence behavior and their overall optimization performance. In particular, we:\n",
    "1. Provide randomzied initial values for the model parameters;\n",
    "2. (Exercise 3.3.1) Ask you to implement the objective / loss function, the negative-log-likelihood;\n",
    "3. (Exercise 3.3.2) Ask you to call optimization algorithm from sklearn to minimize this objective;\n",
    "4. (Exercise 3.3.3) Implement EM algorithm to optimize the objective;\n",
    "5. (Exercise 3.3.4) Observe the convergence behavior in Exercises 3.1.2 and 3.1.3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 3.3.1: GMM with MLE (10 points)\n",
    "\n",
    "In this case, we generate some random data, then ask you to implement the objective function (__log-likelihood__). In this case we use the positive, since EM algorithm is maximizing the log-likelihood, rather than minimizing the negative log-likelihood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data generation\n",
    "np.random.seed(1234)\n",
    "def generate_MoG_data(num_data, means, covariances, weights):\n",
    "    \"\"\" Creates a list of data points \"\"\"\n",
    "    num_clusters = len(weights)\n",
    "    data = []\n",
    "    for i in range(num_data):\n",
    "        #  Use np.random.choice and weights to pick a cluster id greater than or equal to 0 and less than num_clusters\n",
    "        k = np.random.choice(len(weights), 1, p=weights)[0]\n",
    "\n",
    "        # Use np.random.multivariate_normal to create data from this cluster\n",
    "        x = np.random.multivariate_normal(means[k], covariances[k])\n",
    "\n",
    "        data.append(x)\n",
    "    return data\n",
    "\n",
    "# Model parameters\n",
    "init_means = [\n",
    "    [5, 0], # mean of cluster 1\n",
    "    [1, 1], # mean of cluster 2\n",
    "    [0, 5]  # mean of cluster 3\n",
    "]\n",
    "init_covariances = [\n",
    "    [[.5, 0.], [0, .5]], # covariance of cluster 1\n",
    "    [[.92, .38], [.38, .91]], # covariance of cluster 2\n",
    "    [[.5, 0.], [0, .5]]  # covariance of cluster 3\n",
    "]\n",
    "init_weights = [1/4., 1/2., 1/4.]  # weights of each cluster\n",
    "\n",
    "# Generate data\n",
    "np.random.seed(4)\n",
    "data = generate_MoG_data(100, init_means, init_covariances, init_weights)\n",
    "data = np.vstack(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize data\n",
    "plt.plot(data[:,0], data[:,1], 'ko')\n",
    "plt.rcParams.update({'font.size': 16})\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial setup for parameters: `pi`, `mu`, `Sigma` for GMM\n",
    "class Theta(object):\n",
    "    pi = np.empty((0,3))\n",
    "    mu = np.empty((0,3,2))\n",
    "    Sigma = np.empty((0,3,2,2))\n",
    "\n",
    "    def __init__(self, pi, mu, Sigma):\n",
    "        self.pi = pi\n",
    "        self.mu = mu\n",
    "        self.Sigma = Sigma\n",
    "\n",
    "theta_old = Theta(\n",
    "    pi=np.array([0.4, 0.3, 0.3]),\n",
    "    mu=np.array([\n",
    "        [0.0, 0.0],\n",
    "        [3.0, 1.0],\n",
    "        [4.0, 3.0]\n",
    "    ]),\n",
    "    Sigma=np.array([\n",
    "        [[1.0, 0.5],[0.5, 1.0]],\n",
    "        [[1.0, 0.5],[0.5, 1.0]],\n",
    "        [[1.0, 0.5],[0.5, 1.0]]\n",
    "    ])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: implement the objective function, negative log-likelihood, given theta and data x\n",
    "def GMM_objective(theta, x=data):\n",
    "    \"\"\"\n",
    "    `theta`: theta class above\n",
    "    `x`: input data of shape (n,d)\n",
    "    Return the negative log-likelihood of GMM\n",
    "    \"\"\"\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 3.3.2: Optimize the Objective Directly (15 points)\n",
    "\n",
    "In this case we ask you to implement a simple __gradient ascent__ algorithm to maximize the log-likelihood function implemented in Exercise 3.3.1.\n",
    "To do that, you need to implement the gradient of the objective, then update using the standard gradient ascent (not descent, since we are maximizing the objective)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GMM_objective_grad(theta, x=data):\n",
    "    # TODO: calculate and return the gradient of GMM objective w.r.t `pi`, `mu`, `Sigma`\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GMM_gradient_ascent(theta, n_iters=500, x=data):\n",
    "    \"\"\"\n",
    "    TODO:\n",
    "    Call the above two functions to maximize the objective.\n",
    "    Return the updated parameters and the historical record of objective.\n",
    "    \"\"\"\n",
    "    return theta, []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 3.3.3: GMM with EM (20 points)\n",
    "\n",
    "The EM algorithm does not directly deal with the objective function, but instead work on a surrogate. This general line of approach of optimziation for probabilistic models is known as __variational inference__. Now implement the steps of EM algorithm for GMM. Then run the EM algorithm for 500 iterations to obtain the model parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def E_step(theta, data):\n",
    "    \"\"\"\n",
    "    TODO: implement the E-step of the EM algorithm.\n",
    "    Return the updated `theta`.\n",
    "    \"\"\"\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def M_step(theta, data):\n",
    "    \"\"\"\n",
    "    TODO: implement the M-step of the EM algorithm.\n",
    "    Return the updated `theta`.\n",
    "    \"\"\"\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def EM(theta, data, n_iter=500):\n",
    "    \"\"\"\n",
    "    TODO: implement the EM algorithm.\n",
    "    Be sure to call the above two functions `E_step` and `M_step`.\n",
    "    `theta` is the `Theta` class above; `data` is the `data` above.\n",
    "    Return the updated `theta` values and the historical record of objective.\n",
    "    \"\"\"\n",
    "    return theta, []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 3.3.4 (10 points)\n",
    "\n",
    "Based on the saved intermediate values of log-likelihood for the values above, plot and observe the behavior of log-likelihood in each case.\n",
    "Summarize your observations. This execrise is open-ended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Plot the intermediate values of log-likelihood in each case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[TODO: Write your answer here]**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
